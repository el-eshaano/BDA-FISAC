{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "import pyspark  \n",
    "import os  \n",
    "import sys  \n",
    "from pyspark import SparkContext  \n",
    "```  \n",
    "Imports necessary libraries and modules for working with PySpark, including the SparkContext class.  \n",
    "  \n",
    "```python  \n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable  \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable  \n",
    "```  \n",
    "Sets environment variables for PySpark to use the current Python executable for both the worker and driver processes.  \n",
    "  \n",
    "```python  \n",
    "from pyspark.sql import SparkSession  \n",
    "```  \n",
    "Imports the SparkSession class from the pyspark.sql module, which is used to create a Spark session and interact with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_4').getOrCreate()  \n",
    "```  \n",
    "  \n",
    "This line of code creates a new SparkSession object, which is the entry point for working with Apache Spark. Here's a breakdown of what's happening:  \n",
    "  \n",
    "- `SparkSession.builder`: This creates a SparkSession builder object, which allows you to configure various settings for the Spark session.  \n",
    "- `.config(\"spark.driver.memory\", \"16g\")`: This sets the configuration property `spark.driver.memory` to `\"16g\"`, which allocates 16GB of memory for the Spark driver process.  \n",
    "- `.appName('chapter_4')`: This sets the name of the Spark application to `\"chapter_4\"`. This name will appear in the Spark UI and logs.  \n",
    "- `.getOrCreate()`: This method either gets an existing SparkSession or creates a new one if none exists.  \n",
    "  \n",
    "The resulting `spark` object is the entry point for working with Spark, allowing you to create DataFrames, perform transformations, and execute Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_4').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/covtype.data\")  \n",
    "data_without_header.printSchema()  \n",
    "```  \n",
    "  \n",
    "This code reads a CSV file named \"covtype.data\" from the \"data\" directory using Apache Spark's `read` method. The `option(\"inferSchema\", True)` tells Spark to infer the data types of the columns automatically. `option(\"header\", False)` indicates that the CSV file does not have a header row.  \n",
    "  \n",
    "The resulting DataFrame is stored in the variable `data_without_header`. Finally, `data_without_header.printSchema()` prints the schema (column names and data types) of the DataFrame to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/covtype.data\")\n",
    "data_without_header.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is written in Python and uses the PySpark library for working with Apache Spark, a distributed computing framework for big data processing.  \n",
    "  \n",
    "```python  \n",
    "from pyspark.sql.types import DoubleType  \n",
    "from pyspark.sql.functions import col  \n",
    "```  \n",
    "These lines import the `DoubleType` class from `pyspark.sql.types` and the `col` function from `pyspark.sql.functions`. `DoubleType` is used to represent double-precision floating-point numbers, and `col` is used to reference a column in a DataFrame.  \n",
    "  \n",
    "```python\n",
    "colnames = [  \n",
    "    \"Elevation\",  \n",
    "    \"Aspect\",  \n",
    "    \"Slope\",  \n",
    "    \"Horizontal_Distance_To_Hydrology\",  \n",
    "    \"Vertical_Distance_To_Hydrology\",  \n",
    "    \"Horizontal_Distance_To_Roadways\",  \n",
    "    \"Hillshade_9am\",  \n",
    "    \"Hillshade_Noon\",  \n",
    "    \"Hillshade_3pm\",  \n",
    "    \"Horizontal_Distance_To_Fire_Points\"  \n",
    "] + [f\"Wilderness_Area_{i}\" for i in range(4)] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]  \n",
    "```  \n",
    "This line creates a list `colnames` containing column names for a DataFrame. It includes various feature names related to elevation, aspect, slope, distances, hillshade, wilderness areas, soil types, and a target variable \"Cover_Type\".  \n",
    "  \n",
    "```python  \n",
    "data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))  \n",
    "```  \n",
    "This line creates a new DataFrame `data` from an existing DataFrame `data_without_header`. The `toDF` method is used to create a new DataFrame with column names specified in `colnames`. The `withColumn` method is then used to cast the \"Cover_Type\" column to `DoubleType`.  \n",
    "  \n",
    "```python  \n",
    "data.head()  \n",
    "```  \n",
    "This line displays the first few rows of the `data` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "colnames = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\"\n",
    "] + [f\"Wilderness_Area_{i}\" for i in range(4)] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]\n",
    "\n",
    "data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "(train_data, test_data) = data.randomSplit([0.9, 0.1])  \n",
    "```  \n",
    "This line splits the `data` DataFrame into two parts: `train_data` (90% of the data) and `test_data` (10% of the data). The `randomSplit` method is used to randomly split the data into the specified fractions.  \n",
    "  \n",
    "```python  \n",
    "train_data.cache()  \n",
    "test_data.cache()  \n",
    "```  \n",
    "These lines cache the `train_data` and `test_data` DataFrames in memory. Caching is useful when you plan to reuse the data multiple times, as it avoids recomputing the data each time it is accessed, improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = data.randomSplit([0.9, 0.1])\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from pyspark.ml.feature import VectorAssembler` imports the `VectorAssembler` class from the `pyspark.ml.feature` module. This class is used to combine multiple input columns into a single vector column.  \n",
    "  \n",
    "```python  \n",
    "input_cols = colnames[:-1]  \n",
    "```  \n",
    "This line creates a list `input_cols` containing all column names from the `colnames` list except the last one. These columns will be used as input features for the `VectorAssembler`.  \n",
    "  \n",
    "```python  \n",
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")  \n",
    "```  \n",
    "This line creates an instance of the `VectorAssembler` class with `input_cols` as the input columns and `\"featureVector\"` as the name of the output column containing the assembled vectors.  \n",
    "  \n",
    "```python  \n",
    "assembled_train_data = vector_assembler.transform(train_data)  \n",
    "```  \n",
    "This line applies the `vector_assembler` transformation to the `train_data` DataFrame, creating a new DataFrame `assembled_train_data` with the `\"featureVector\"` column added.  \n",
    "  \n",
    "```python  \n",
    "assembled_train_data.select(\"featureVector\").show(truncate = False)  \n",
    "```  \n",
    "This line selects the `\"featureVector\"` column from the `assembled_train_data` DataFrame and prints its contents without truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols = colnames[:-1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "assembled_train_data = vector_assembler.transform(train_data)\n",
    "assembled_train_data.select(\"featureVector\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "from pyspark.ml.classification import DecisionTreeClassifier  \n",
    "```  \n",
    "Imports the DecisionTreeClassifier class from the pyspark.ml.classification module. This class is used to create a decision tree model for classification tasks.  \n",
    "  \n",
    "```python  \n",
    "classifier = DecisionTreeClassifier(seed = 1234, labelCol=\"Cover_Type\",  \n",
    "                                    featuresCol=\"featureVector\",  \n",
    "                                    predictionCol=\"prediction\")  \n",
    "```  \n",
    "Creates an instance of the DecisionTreeClassifier with the following parameters:  \n",
    "- `seed`: Sets the random seed for reproducibility.  \n",
    "- `labelCol`: Specifies the name of the column containing the labels/target variable.  \n",
    "- `featuresCol`: Specifies the name of the column containing the feature vectors.  \n",
    "- `predictionCol`: Specifies the name of the column where the predicted labels will be stored.  \n",
    "  \n",
    "```python  \n",
    "model = classifier.fit(assembled_train_data)  \n",
    "```  \n",
    "Trains the decision tree model on the `assembled_train_data` DataFrame.  \n",
    "  \n",
    "```python  \n",
    "print(model.toDebugString)  \n",
    "```  \n",
    "Prints the debug string representation of the trained decision tree model, which can be useful for understanding the structure and parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(seed = 1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"featureVector\",\n",
    "predictionCol=\"prediction\")\n",
    "model = classifier.fit(assembled_train_data)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "import pandas as pd  \n",
    "  \n",
    "pd.DataFrame(  \n",
    "    model.featureImportances.toArray(),   \n",
    "    index=input_cols,   \n",
    "    columns=['importance']  \n",
    ").sort_values(by=\"importance\", ascending=False)  \n",
    "```  \n",
    "  \n",
    "This code creates a pandas DataFrame from the feature importances of a machine learning model. Here's a breakdown:  \n",
    "  \n",
    "- `import pandas as pd`: Imports the pandas library, which provides data manipulation and analysis tools.  \n",
    "- `model.featureImportances.toArray()`: Retrieves the feature importances from the `model` object and converts them to an array.  \n",
    "- `index=input_cols`: Sets the row labels (index) of the DataFrame to the `input_cols` variable, which likely contains the names of the input features.  \n",
    "- `columns=['importance']`: Sets the column name of the DataFrame to 'importance'.  \n",
    "- `.sort_values(by=\"importance\", ascending=False)`: Sorts the DataFrame in descending order based on the 'importance' column.  \n",
    "  \n",
    "The resulting DataFrame will have the feature names as row labels and their corresponding importance values in the 'importance' column, sorted from highest to lowest importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(\n",
    "    model.featureImportances.toArray(), \n",
    "    index=input_cols, columns=['importance']).sort_values(by=\"importance\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "predictions = model.transform(assembled_train_data)  \n",
    "```  \n",
    "This line applies the trained model to the assembled training data to generate predictions.  \n",
    "  \n",
    "```python  \n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(10, truncate = False)  \n",
    "```  \n",
    "This line selects the columns \"Cover_Type\" (the target variable), \"prediction\" (the predicted class), and \"probability\" (the probability of the predicted class) from the `predictions` DataFrame, and displays the first 10 rows without truncating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(assembled_train_data)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(10, truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator  \n",
    "```  \n",
    "Imports the `MulticlassClassificationEvaluator` class from the `pyspark.ml.evaluation` module. This class is used to evaluate the performance of multiclass classification models.  \n",
    "  \n",
    "```python  \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\")  \n",
    "```  \n",
    "Creates an instance of the `MulticlassClassificationEvaluator` class, specifying the column names for the true labels (`\"Cover_Type\"`) and predicted labels (`\"prediction\"`).  \n",
    "  \n",
    "```python  \n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)  \n",
    "```  \n",
    "Sets the evaluation metric to \"accuracy\" and evaluates the predictions on the given `predictions` DataFrame. This calculates the overall accuracy of the multiclass classification model.  \n",
    "  \n",
    "```python  \n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)  \n",
    "```  \n",
    "Sets the evaluation metric to \"f1\" (F1 score) and evaluates the predictions on the same `predictions` DataFrame. This calculates the weighted average of the F1 scores for each class in the multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\")\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "confusion_matrix = predictions.groupBy(\"Cover_Type\").pivot(\"prediction\", range(1,8)).count().na.fill(0.0).orderBy(\"Cover_Type\")  \n",
    "confusion_matrix.show()  \n",
    "```  \n",
    "  \n",
    "This code calculates the confusion matrix for a multi-class classification problem and displays it.  \n",
    "  \n",
    "`predictions.groupBy(\"Cover_Type\")`: Groups the predictions DataFrame by the \"Cover_Type\" column, which likely represents the true labels.  \n",
    "  \n",
    "`.pivot(\"prediction\", range(1,8))`: Pivots the DataFrame to create a column for each possible prediction value (1 to 7). This is done to create a contingency table.  \n",
    "  \n",
    "`.count()`: Counts the number of occurrences for each combination of \"Cover_Type\" and \"prediction\" values.  \n",
    "  \n",
    "`.na.fill(0.0)`: Fills any missing (NaN) values with 0.0, assuming that missing values represent zero occurrences.  \n",
    "  \n",
    "`.orderBy(\"Cover_Type\")`: Sorts the resulting DataFrame by the \"Cover_Type\" column.  \n",
    "  \n",
    "`confusion_matrix.show()`: Displays the resulting confusion matrix DataFrame.  \n",
    "  \n",
    "The confusion matrix shows the number of instances that were correctly and incorrectly classified for each class. The diagonal elements represent the correctly classified instances, while the off-diagonal elements represent the misclassified instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = predictions.groupBy(\"Cover_Type\").pivot(\"prediction\", range(1,8)).count().na.fill(0.0).orderBy(\"Cover_Type\")\n",
    "confusion_matrix.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `class_probabilities` that takes a PySpark DataFrame `data` as input and calculates the proportion of each class in the data. Here's a breakdown of the code:  \n",
    "  \n",
    "```python  \n",
    "def class_probabilities(data):  \n",
    "    total = data.count() # Get the total number of rows in the DataFrame  \n",
    "    return data.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").select(col(\"count\").cast(DoubleType())).withColumn(\"count_proportion\", col(\"count\")/total).select(\"count_proportion\").collect()  \n",
    "```  \n",
    "  \n",
    "The function is then called twice with different DataFrames `train_data` and `test_data` to calculate the class probabilities for each dataset:  \n",
    "  \n",
    "```python  \n",
    "train_prior_probabilities = class_probabilities(train_data)  \n",
    "test_prior_probabilities = class_probabilities(test_data)  \n",
    "```  \n",
    "  \n",
    "The resulting lists of Row objects are converted to lists of floats:  \n",
    "  \n",
    "```python  \n",
    "train_prior_probabilities = [p[0] for p in train_prior_probabilities]  \n",
    "test_prior_probabilities = [p[0] for p in test_prior_probabilities]  \n",
    "```  \n",
    "  \n",
    "Finally, the sum of the products of corresponding elements from `train_prior_probabilities` and `test_prior_probabilities` is calculated using a list comprehension and the `zip` function:  \n",
    "  \n",
    "```python  \n",
    "sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities, test_prior_probabilities)])  \n",
    "```  \n",
    "  \n",
    "This code calculates the class probabilities for two datasets and then computes a metric based on the product of corresponding class probabilities from the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "def class_probabilities(data):\n",
    "    total = data.count()\n",
    "    return data.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").select(col(\"count\").cast(DoubleType())).withColumn(\"count_proportion\", col(\"count\")/total).select(\"count_proportion\").collect()\n",
    "\n",
    "train_prior_probabilities = class_probabilities(train_data)\n",
    "test_prior_probabilities = class_probabilities(test_data)\n",
    "\n",
    "train_prior_probabilities = [p[0] for p in train_prior_probabilities]\n",
    "test_prior_probabilities = [p[0] for p in test_prior_probabilities]\n",
    "sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities, test_prior_probabilities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "from pyspark.ml import Pipeline  \n",
    "```  \n",
    "Imports the `Pipeline` class from the `pyspark.ml` module, which is used to chain multiple Transformer and Estimator objects in a sequence.  \n",
    "  \n",
    "```python  \n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")  \n",
    "```  \n",
    "Creates a `VectorAssembler` object that combines multiple input columns into a single vector column named \"featureVector\". The `input_cols` parameter specifies the list of input column names to be combined.  \n",
    "  \n",
    "```python  \n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\", featuresCol=\"featureVector\", predictionCol=\"prediction\")  \n",
    "```  \n",
    "Creates a `DecisionTreeClassifier` object with the specified parameters:  \n",
    "- `seed=1234`: Sets the random seed for reproducibility.  \n",
    "- `labelCol=\"Cover_Type\"`: Specifies the name of the label column.  \n",
    "- `featuresCol=\"featureVector\"`: Specifies the name of the feature vector column created by the `VectorAssembler`.  \n",
    "- `predictionCol=\"prediction\"`: Specifies the name of the output column for predicted labels.  \n",
    "  \n",
    "```python  \n",
    "pipeline = Pipeline(stages=[assembler, classifier])  \n",
    "```  \n",
    "Creates a `Pipeline` object that chains the `assembler` and `classifier` stages together. The `Pipeline` will first apply the `VectorAssembler` to combine input columns, and then apply the `DecisionTreeClassifier` on the resulting feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\", featuresCol=\"featureVector\", predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This markdown documentation explains the provided Python code:  \n",
    "  \n",
    "The code imports the `ParamGridBuilder` class from the `pyspark.ml.tuning` module. This class is used to create a grid of hyperparameters for model tuning.  \n",
    "  \n",
    "```python  \n",
    "from pyspark.ml.tuning import ParamGridBuilder  \n",
    "```  \n",
    "  \n",
    "The `ParamGridBuilder` is then used to create a `paramGrid` object, which specifies the hyperparameters and their values to be explored during the tuning process.  \n",
    "  \n",
    "```python  \n",
    "paramGrid = ParamGridBuilder().addGrid(classifier.impurity, [\"gini\", \"entropy\"]).addGrid(classifier.maxDepth, [1, 20]).addGrid(classifier.maxBins, [40, 300]).addGrid(classifier.minInfoGain, [0.0, 0.05]).build()  \n",
    "```  \n",
    "  \n",
    "The `addGrid` method is called multiple times to add different hyperparameters and their respective values to the grid. In this case, the hyperparameters being tuned are:  \n",
    "  \n",
    "- `impurity`: Either \"gini\" or \"entropy\" for the impurity criterion.  \n",
    "- `maxDepth`: Maximum depth of the decision tree, with values 1 and 20.  \n",
    "- `maxBins`: Maximum number of bins used for splitting features, with values 40 and 300.  \n",
    "- `minInfoGain`: Minimum information gain for a split to be considered, with values 0.0 and 0.05.  \n",
    "  \n",
    "Finally, the `build` method is called to create the `paramGrid` object.  \n",
    "  \n",
    "The code also creates a `MulticlassClassificationEvaluator` object, which is used to evaluate the performance of a multiclass classification model.  \n",
    "  \n",
    "```python  \n",
    "multiclassEval = MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")  \n",
    "```  \n",
    "  \n",
    "The `setLabelCol` method specifies the column containing the true labels, which is \"Cover_Type\" in this case. The `setPredictionCol` method specifies the column containing the predicted labels, which is \"prediction\". The `setMetricName` method sets the evaluation metric to be used,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder().addGrid(classifier.impurity, [\"gini\", \"entropy\"]).addGrid(classifier.maxDepth, [1, 20]).addGrid(classifier.maxBins, [40, 300]).addGrid(classifier.minInfoGain, [0.0, 0.05]).build()\n",
    "multiclassEval = MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "from pyspark.ml.tuning import TrainValidationSplit  \n",
    "validator = TrainValidationSplit(  \n",
    "    seed=1234,  \n",
    "    estimator=pipeline,  \n",
    "    evaluator=multiclassEval,  \n",
    "    estimatorParamMaps=paramGrid,  \n",
    "    trainRatio=0.9  \n",
    ")  \n",
    "validator_model = validator.fit(train_data)  \n",
    "```  \n",
    "  \n",
    "This code sets up a `TrainValidationSplit` object for model tuning in PySpark. It splits the training data into separate training and validation sets, allowing for hyperparameter tuning and model evaluation.  \n",
    "  \n",
    "- `from pyspark.ml.tuning import TrainValidationSplit` imports the `TrainValidationSplit` class from PySpark's tuning module.  \n",
    "- `TrainValidationSplit` is initialized with the following parameters:  \n",
    "  - `seed=1234` sets a random seed for reproducibility.  \n",
    "  - `estimator=pipeline` specifies the machine learning pipeline to be tuned.  \n",
    "  - `evaluator=multiclassEval` sets the evaluation metric for the tuning process (e.g., accuracy, F1-score).  \n",
    "  - `estimatorParamMaps=paramGrid` provides a grid of hyperparameter values to be evaluated.  \n",
    "  - `trainRatio=0.9` specifies that 90% of the data should be used for training, and the remaining 10% for validation.  \n",
    "- `validator_model = validator.fit(train_data)` trains the `TrainValidationSplit` object on the `train_data` dataset, producing a tuned model.  \n",
    "  \n",
    "The resulting `validator_model` can be used for making predictions or further analysis, having been tuned on the validation set for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "validator = TrainValidationSplit(\n",
    "    seed=1234,\n",
    "    estimator=pipeline,\n",
    "    evaluator=multiclassEval,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    trainRatio=0.9\n",
    ")\n",
    "validator_model = validator.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "from pprint import pprint  \n",
    "best_model = validator_model.bestModel  \n",
    "```  \n",
    "Imports the `pprint` module for pretty printing and assigns the best model from the `validator_model` object to the `best_model` variable.  \n",
    "  \n",
    "```python  \n",
    "pprint(best_model.stages[1].extractParamMap())  \n",
    "```  \n",
    "Pretty prints the parameter map of the second stage in the `best_model` pipeline.  \n",
    "  \n",
    "```python  \n",
    "multiclassEval.evaluate(best_model.transform(test_data))  \n",
    "```  \n",
    "Evaluates the `best_model` on the `test_data` using the `multiclassEval` evaluator, which likely calculates metrics for a multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "best_model = validator_model.bestModel\n",
    "pprint(best_model.stages[1].extractParamMap())\n",
    "\n",
    "multiclassEval.evaluate(best_model.transform(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "validator_model = validator.fit(train_data)  \n",
    "```  \n",
    "Fits a validator model on the training data `train_data`.  \n",
    "  \n",
    "```python  \n",
    "metrics = validator_model.validationMetrics  \n",
    "params = validator_model.getEstimatorParamMaps()  \n",
    "```  \n",
    "Retrieves the validation metrics and estimator parameter maps from the fitted validator model.  \n",
    "  \n",
    "```python  \n",
    "metrics_and_params = list(zip(metrics, params))  \n",
    "metrics_and_params.sort(key=lambda x: x[0], reverse=True)  \n",
    "```  \n",
    "Combines the metrics and parameter maps into a list of tuples, and sorts the list in descending order based on the metric values.  \n",
    "  \n",
    "```python  \n",
    "metrics.sort(reverse=True)  \n",
    "print(metrics[0])  \n",
    "```  \n",
    "Sorts the metrics list in descending order, and prints the highest metric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_model = validator.fit(train_data)\n",
    "metrics = validator_model.validationMetrics\n",
    "params = validator_model.getEstimatorParamMaps()\n",
    "metrics_and_params = list(zip(metrics, params))\n",
    "metrics_and_params.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "metrics.sort(reverse=True)\n",
    "print(metrics[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from pyspark.sql.functions import udf`  \n",
    "Imports the `udf` function from the `pyspark.sql.functions` module, which is used to create user-defined functions in PySpark.  \n",
    "  \n",
    "`from pyspark.sql.types import IntegerType`  \n",
    "Imports the `IntegerType` class from the `pyspark.sql.types` module, which is used to specify the return type of the user-defined function.  \n",
    "  \n",
    "```python  \n",
    "def unencode_one_hot(data):  \n",
    "    wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]  \n",
    "    wilderness_assembler = VectorAssembler().setInputCols(wilderness_cols).setOutputCol(\"wilderness\")  \n",
    "    unhot_udf = udf(lambda v: v.toArray().tolist().index(1))  \n",
    "    with_wilderness = wilderness_assembler.transform(data).drop(*wilderness_cols).withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")).cast(IntegerType()))  \n",
    "```  \n",
    "This part of the code deals with the \"Wilderness_Area\" columns. It creates a list of column names `wilderness_cols`, then uses `VectorAssembler` to combine these columns into a single vector column named \"wilderness\". It then creates a user-defined function `unhot_udf` that takes a vector and returns the index of the element with value 1 (assuming it's a one-hot encoded vector). This function is applied to the \"wilderness\" column, and the result is cast to `IntegerType`.  \n",
    "  \n",
    "```python  \n",
    "    soil_cols = ['Soil_Type_' + str(i) for i in range(40)]  \n",
    "    soil_assembler = VectorAssembler().setInputCols(soil_cols).setOutputCol(\"soil\")  \n",
    "    with_soil = soil_assembler.transform(with_wilderness).drop(*soil_cols).withColumn(\"soil\", unhot_udf(col(\"soil\")).cast(IntegerType()))  \n",
    "```  \n",
    "This part of the code does the same thing as the previous part, but for the \"Soil_Type\" columns. It creates a list of column names `soil_cols`, combines them into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def unencode_one_hot(data):\n",
    "    wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n",
    "    wilderness_assembler = VectorAssembler().setInputCols(wilderness_cols).setOutputCol(\"wilderness\")\n",
    "    unhot_udf = udf(lambda v: v.toArray().tolist().index(1))\n",
    "    with_wilderness = wilderness_assembler.transform(data).drop(*wilderness_cols).withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")).cast(IntegerType()))\n",
    "    soil_cols = ['Soil_Type_' + str(i) for i in range(40)]\n",
    "    soil_assembler = VectorAssembler().setInputCols(soil_cols).setOutputCol(\"soil\")\n",
    "    with_soil = soil_assembler.transform(with_wilderness).drop(*soil_cols).withColumn(\"soil\", unhot_udf(col(\"soil\")).cast(IntegerType()))\n",
    "    return with_soil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs the following tasks:  \n",
    "  \n",
    "1. `unenc_train_data = unencode_one_hot(train_data)` converts the one-hot encoded `train_data` into a more compact format.  \n",
    "2. `unenc_train_data.printSchema()` prints the schema of the `unenc_train_data` DataFrame.  \n",
    "3. `unenc_train_data.groupBy('wilderness').count().show()` groups the data by the 'wilderness' column and counts the number of rows for each group.  \n",
    "  \n",
    "```python  \n",
    "from pyspark.ml.feature import VectorIndexer  \n",
    "cols = unenc_train_data.columns  \n",
    "input_cols = [c for c in cols if c!='Cover_Type']  \n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")  \n",
    "```  \n",
    "This code creates a `VectorAssembler` object that combines all columns except 'Cover_Type' into a single vector column named 'featureVector'.  \n",
    "  \n",
    "```python  \n",
    "indexer = VectorIndexer().setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")  \n",
    "classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").setFeaturesCol(\"indexedVector\").setPredictionCol(\"prediction\")  \n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])  \n",
    "```  \n",
    "This code creates a `VectorIndexer` object that converts the 'featureVector' column into a numerical vector with a maximum of 40 categories. It then creates a `DecisionTreeClassifier` object with 'Cover_Type' as the label column, 'indexedVector' as the features column, and 'prediction' as the prediction column. Finally, it creates a `Pipeline` object that combines the `assembler`, `indexer`, and `classifier` stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unenc_train_data = unencode_one_hot(train_data)\n",
    "unenc_train_data.printSchema()\n",
    "unenc_train_data.groupBy('wilderness').count().show()\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").setFeaturesCol(\"indexedVector\").setPredictionCol(\"prediction\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "from pyspark.ml.classification import RandomForestClassifier  \n",
    "  \n",
    "# Create a RandomForestClassifier instance  \n",
    "classifier = RandomForestClassifier(seed=1234, # Set the seed for reproducibility  \n",
    "                                    labelCol=\"Cover_Type\", # Name of the label column  \n",
    "                                    featuresCol=\"indexedVector\", # Name of the features column  \n",
    "                                    predictionCol=\"prediction\") # Name of the prediction column  \n",
    "```  \n",
    "  \n",
    "This code imports the `RandomForestClassifier` class from the `pyspark.ml.classification` module. It then creates an instance of the `RandomForestClassifier` with the following parameters:  \n",
    "  \n",
    "- `seed=1234`: Sets the seed for the random number generator to ensure reproducibility.  \n",
    "- `labelCol=\"Cover_Type\"`: Specifies the name of the column containing the labels/target variable.  \n",
    "- `featuresCol=\"indexedVector\"`: Specifies the name of the column containing the feature vectors.  \n",
    "- `predictionCol=\"prediction\"`: Specifies the name of the column where the predicted labels will be stored.  \n",
    "  \n",
    "The `RandomForestClassifier` is a supervised learning algorithm used for classification tasks. It constructs multiple decision trees and combines their predictions to improve accuracy and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.from pyspark.ml.classification import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\", featuresCol=\"indexedVector\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is written in Python and uses the PySpark library for machine learning tasks. Here's a breakdown of what the code does:  \n",
    "  \n",
    "```python  \n",
    "cols = unenc_train_data.columns  \n",
    "input_cols = [c for c in cols if c!='Cover_Type']  \n",
    "```  \n",
    "This extracts the column names from the `unenc_train_data` DataFrame and creates a list `input_cols` containing all column names except 'Cover_Type'.  \n",
    "  \n",
    "```python  \n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")  \n",
    "indexer = VectorIndexer.setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")  \n",
    "```  \n",
    "These lines create two PySpark Transformer objects: `assembler` and `indexer`. `assembler` combines the input columns into a single vector column named \"featureVector\". `indexer` encodes the vector column into a numerical format with a maximum of 40 categories.  \n",
    "  \n",
    "```python  \n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])  \n",
    "```  \n",
    "This creates a `Pipeline` object that chains the `assembler`, `indexer`, and `classifier` (not shown) together.  \n",
    "  \n",
    "```python  \n",
    "paramGrid = ParamGridBuilder().addGrid(classifier.impurity, [\"gini\", \"entropy\"]).addGrid(classifier.maxDepth, [1, 20]).addGrid(classifier.maxBins, [40, 300]).addGrid(classifier.minInfoGain, [0.0, 0.05]).build()  \n",
    "```  \n",
    "This creates a `ParamGrid` object that defines a grid of hyperparameters to be tested for the `classifier` model.  \n",
    "  \n",
    "```python  \n",
    "multiclassEval = MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")  \n",
    "```  \n",
    "This creates a `MulticlassClassificationEvaluator` object that will evaluate the model's performance using the accuracy metric.  \n",
    "  \n",
    "```python  \n",
    "validator = TrainValidationSplit(  \n",
    "    seed=1234,  \n",
    "    estimator=pipeline,  \n",
    "    evaluator=multiclassEval,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer.setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])\n",
    "paramGrid = ParamGridBuilder().addGrid(classifier.impurity, [\"gini\", \"entropy\"]).addGrid(classifier.maxDepth, [1, 20]).addGrid(classifier.maxBins, [40, 300]).addGrid(classifier.minInfoGain, [0.0, 0.05]).build()\n",
    "multiclassEval = MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "validator = TrainValidationSplit(\n",
    "    seed=1234,\n",
    "    estimator=pipeline,\n",
    "    evaluator=multiclassEval,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    trainRatio=0.9\n",
    ")\n",
    "validator_model = validator.fit(unenc_train_data)\n",
    "best_model = validator_model.bestModel\n",
    "\n",
    "forest_model = best_model.stages[2]\n",
    "feature_importance_list = list(zip(input_cols,\n",
    "forest_model.featureImportances.toArray()))\n",
    "feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "pprint(feature_importance_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
