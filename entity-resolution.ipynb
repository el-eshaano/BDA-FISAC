{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary libraries and modules**  \n",
    "- `pyspark`: the main PySpark library  \n",
    "- `os` and `sys`: used for setting environment variables  \n",
    "  \n",
    "**Setting environment variables**  \n",
    "- `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` are set to the current Python executable path  \n",
    "  \n",
    "**Importing `SparkSession` from `pyspark.sql`**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Creating a `SparkSession` instance**  \n",
    "- `SparkSession.builder`: used to create a new `SparkSession`  \n",
    "- `.config(\"spark.driver.memory\", \"16g\")`: sets the driver memory to 16GB  \n",
    "- `.appName('chapter_2')`: sets the application name to \"chapter_2\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_2').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading a CSV file without specifying options**  \n",
    "- `spark.read.csv(\"linkage/block_1.csv\")`: reads the CSV file \"block_1.csv\" from the \"linkage\" directory  \n",
    "- `.show(2)`: displays the first 2 rows of the DataFrame  \n",
    "  \n",
    "\n",
    "**Reading a CSV file with specified options**  \n",
    "- `.option(\"header\", \"true\")`: treats the first row as the header  \n",
    "- `.option(\"nullValue\", \"?\")`: sets the null value placeholder to \"?\"  \n",
    "- `.option(\"inferSchema\", \"true\")`: infers the data types of columns automatically  \n",
    "- `.printSchema()`: prints the schema of the DataFrame  \n",
    "- `.show(5)`: displays the first 5 rows of the DataFrame  \n",
    "- `.count()`: returns the number of rows in the DataFrame  \n",
    "\n",
    "**Caching the DataFrame in memory**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = spark.read.csv(\"linkage/block_1.csv\")\n",
    "prev.show(2)\n",
    "\n",
    "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").option(\"inferSchema\", \"true\").csv(\"linkage/block_1.csv\")\n",
    "parsed.printSchema()\n",
    "parsed.show(5)\n",
    "print(parsed.count())\n",
    "\n",
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the `col` function from `pyspark.sql.functions`**  \n",
    "- `col`: used to refer to a column in a DataFrame  \n",
    "  \n",
    "**Grouping the DataFrame by a column and counting the occurrences**  \n",
    "- `.groupBy(\"is_match\")`: groups the DataFrame by the \"is_match\" column  \n",
    "- `.count()`: counts the number of rows in each group  \n",
    "- `.orderBy(col(\"count\").desc())`: sorts the resulting DataFrame by the \"count\" column in descending order  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a temporary view of the DataFrame**  \n",
    "- `.createOrReplaceTempView(\"linkage\")`: creates a temporary view named \"linkage\" from the `parsed` DataFrame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running a SQL query on the temporary view**  \n",
    "- `spark.sql()`: executes a SQL query on the temporary view \"linkage\"  \n",
    "- The SQL query:  \n",
    "  - Selects the \"is_match\" column and counts the number of rows for each distinct value using `COUNT(*)`  \n",
    "  - Groups the result by the \"is_match\" column using `GROUP BY`  \n",
    "  - Orders the result by the count column alias \"cnt\" in descending order using `ORDER BY cnt DESC`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT is_match, COUNT(*) cnt\n",
    "FROM linkage\n",
    "GROUP BY is_match\n",
    "ORDER BY cnt DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating summary statistics for the DataFrame**  \n",
    "- `.describe()`: computes summary statistics for numeric and string columns in the DataFrame  \n",
    "- The resulting DataFrame contains statistics like count, mean, standard deviation, min, and max for each column  \n",
    "- `.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\")`: selects specific columns from the summary DataFrame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = parsed.describe()\n",
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Filtering the DataFrame to get only the matched records**  \n",
    "- `.where(\"is_match = true\")`: filters the DataFrame to include only rows where the \"is_match\" column is true  \n",
    "- `.describe()`: generates summary statistics for the filtered DataFrame (`matches`)  \n",
    "   \n",
    "**Filtering the DataFrame to get only the non-matched records**  \n",
    "- `.filter(col(\"is_match\") == False)`: filters the DataFrame to include only rows where the \"is_match\" column is false  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = parsed.where(\"is_match = true\")\n",
    "match_summary = matches.describe()\n",
    "misses = parsed.filter(col(\"is_match\") == False)\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting the summary DataFrame to a pandas DataFrame**  \n",
    "- `.toPandas()`: converts the PySpark DataFrame (`summary`) to a pandas DataFrame  \n",
    "- The resulting pandas DataFrame is assigned to the variable `summary_p`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying the first few rows of the pandas DataFrame**  \n",
    "- `.head()`: shows the first 5 rows of the pandas DataFrame (`summary_p`) by default  \n",
    "  \n",
    "**Checking the dimensions of the pandas DataFrame**  \n",
    "- `.shape`: returns a tuple representing the dimensions of the DataFrame (`summary_p`)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p.head()\n",
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary_p = summary_p.set_index('summary').transpose().reset_index()   \n",
    "**Reshaping the pandas DataFrame**  \n",
    "- `.set_index('summary')`: sets the 'summary' column as the index of the DataFrame  \n",
    "- `.transpose()`: transposes the DataFrame, swapping rows and columns  \n",
    "- `.reset_index()`: resets the index, moving the index to a regular column  \n",
    "  \n",
    "**Renaming a column in the pandas DataFrame**  \n",
    "- `.rename(columns={'index':'field'})`: renames the 'index' column to 'field'  \n",
    "\n",
    "**Removing the column index name**  \n",
    "- `.rename_axis(None, axis=1)`: removes the name of the column index (axis=1) by setting it to `None`  \n",
    "  \n",
    "**Checking the dimensions of the modified pandas DataFrame**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "summary_p = summary_p.rename_axis(None, axis=1)\n",
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a PySpark DataFrame from a pandas DataFrame**  \n",
    "- `spark.createDataFrame(summary_p)`: creates a new PySpark DataFrame (`summaryT`) from the pandas DataFrame (`summary_p`)  \n",
    "- This allows for using PySpark operations and functions on the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryT = spark.createDataFrame(summary_p)\n",
    "summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Importing the `DoubleType` from `pyspark.sql.types`**  \n",
    "- `DoubleType`: represents a double precision floating point number data type in PySpark  \n",
    "  \n",
    "**Casting columns to `DoubleType`**  \n",
    "- Iterates over each column in the DataFrame (`summaryT`)  \n",
    "- Skips the 'field' column using `if c == 'field': continue`  \n",
    "- For each remaining column:  \n",
    "  - `.withColumn(c, summaryT[c].cast(DoubleType()))`: casts the column to `DoubleType` using `.cast()`  \n",
    "  - Overwrites the DataFrame (`summaryT`) with the updated column data type  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "for c in summaryT.columns:\n",
    "    if c == 'field':\n",
    "        continue\n",
    "    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n",
    "summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a function to pivot and transform a summary DataFrame**  \n",
    "- `pivot_summary(desc)`: takes a PySpark DataFrame (`desc`) as input and returns a transformed DataFrame  \n",
    "- The function performs the following steps:  \n",
    "  1. Converts the PySpark DataFrame to a pandas DataFrame using `.toPandas()`  \n",
    "  2. Transposes the pandas DataFrame:  \n",
    "     - Sets the 'summary' column as the index using `.set_index('summary')`  \n",
    "     - Transposes the DataFrame using `.transpose()`  \n",
    "     - Resets the index using `.reset_index()`  \n",
    "  3. Renames the 'index' column to 'field' using `.rename(columns={'index':'field'})`  \n",
    "  4. Removes the column index name using `.rename_axis(None, axis=1)`  \n",
    "  5. Converts the transformed pandas DataFrame back to a PySpark DataFrame using `spark.createDataFrame(desc_p)`  \n",
    "  6. Casts the metric columns to `DoubleType`:  \n",
    "     - Iterates over each column in the DataFrame  \n",
    "     - Skips the 'field' column  \n",
    "     - Casts the remaining columns to `DoubleType` using `.withColumn(c, descT[c].cast(DoubleType()))`  \n",
    "  7. Returns the transformed PySpark DataFrame (`descT`)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "def pivot_summary(desc):\n",
    "    # convert to pandas dataframe\n",
    "    desc_p = desc.toPandas()\n",
    "    # transpose\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    # convert to Spark dataframe\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    # convert metric columns to double from string\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying the `pivot_summary` function to the `match_summary` and `miss_summary` DataFrames**  \n",
    "- `match_summaryT = pivot_summary(match_summary)`: calls the `pivot_summary` function with the `match_summary` DataFrame as input  \n",
    "  - The function pivots and transforms the `match_summary` DataFrame  \n",
    "  - The resulting transformed DataFrame is assigned to `match_summaryT`  \n",
    "- `miss_summaryT = pivot_summary(miss_summary)`: calls the `pivot_summary` function with the `miss_summary` DataFrame as input  \n",
    "  - The function pivots and transforms the `miss_summary` DataFrame  \n",
    "  - The resulting transformed DataFrame is assigned to `miss_summaryT`  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating temporary views for the transformed DataFrames**  \n",
    "- `.createOrReplaceTempView(\"match_desc\")`: creates a temporary view named \"match_desc\" from the `match_summaryT` DataFrame  \n",
    "- `.createOrReplaceTempView(\"miss_desc\")`: creates a temporary view named \"miss_desc\" from the `miss_summaryT` DataFrame  \n",
    "- The temporary views allow running SQL queries on the DataFrames using `spark.sql()`  \n",
    "  \n",
    "\n",
    "**Running a SQL query on the temporary views**  \n",
    "- `spark.sql()`: executes a SQL query on the temporary views \"match_desc\" and \"miss_desc\"  \n",
    "- The SQL query:  \n",
    "  - Selects the \"field\" column from the \"match_desc\" view aliased as \"a\"  \n",
    "  - Calculates the total count by adding the \"count\" columns from both views using `a.count + b.count`  \n",
    "  - Calculates the difference in means between the views using `a.mean - b.mean`  \n",
    "  - Joins the \"match_desc\" and \"miss_desc\" views based on the \"field\" column using `INNER JOIN`  \n",
    "  - Filters out the \"id_1\" and \"id_2\" fields using `WHERE a.field NOT IN (\"id_1\", \"id_2\")`  \n",
    "  - Orders the result by the calculated delta in descending order and then by the total count in descending order using `ORDER BY delta DESC, total DESC`  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView(\"match_desc\")\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_desc\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n",
    "FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n",
    "WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
    "ORDER BY delta DESC, total DESC\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Creating a sum expression for selected features**  \n",
    "- `good_features`: a list of selected feature column names  \n",
    "- `\" + \".join(good_features)`: concatenates the feature column names with \" + \" as the separator  \n",
    "- The resulting `sum_expression` is a string that represents the sum of the selected features  \n",
    "  \n",
    "**Displaying the sum expression**  \n",
    "- The `sum_expression` variable contains the string representation of the sum of the selected features  \n",
    "- The expression will be in the format: \"cmp_lname_c1 + cmp_plz + cmp_by + cmp_bd + cmp_bm\"  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "sum_expression = \" + \".join(good_features)\n",
    "sum_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the `expr` function from `pyspark.sql.functions`**  \n",
    "- `expr`: used to create a column expression from a string  \n",
    "  \n",
    "**Creating a new DataFrame with a score column**  \n",
    "- `.fillna(0, subset=good_features)`: fills null values with 0 for the selected feature columns  \n",
    "- `.withColumn('score', expr(sum_expression))`: adds a new column named 'score' calculated using the `sum_expression`  \n",
    "  - `expr(sum_expression)`: evaluates the `sum_expression` string as a column expression  \n",
    "- `.select('score', 'is_match')`: selects only the 'score' and 'is_match' columns from the resulting DataFrame  \n",
    "- The resulting DataFrame is assigned to the variable `scored`  \n",
    "   \n",
    "**Displaying the `scored` DataFrame**  \n",
    "- `.show()`: displays the first 20 rows of the `scored` DataFrame  \n",
    "- The output will show the 'score' and 'is_match' columns for each row  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "scored = parsed.fillna(0, subset=good_features).withColumn('score', expr(sum_expression)).select('score', 'is_match')\n",
    "\n",
    "scored.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a function to create a cross-tabulation DataFrame**  \n",
    "- `crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame`: takes a DataFrame (`scored`) and a threshold value (`t`) as input and returns a cross-tabulation DataFrame  \n",
    "- The function performs the following steps:  \n",
    "  1. `.selectExpr(f\"score >= {t} as above\", \"is_match\")`: selects the 'is_match' column and creates a new boolean column named 'above' indicating whether the 'score' is greater than or equal to the threshold `t`  \n",
    "  2. `.groupBy(\"above\")`: groups the DataFrame by the 'above' column  \n",
    "  3. `.pivot(\"is_match\", (\"true\", \"false\"))`: pivots the DataFrame based on the 'is_match' column, creating columns for 'true' and 'false' values  \n",
    "  4. `.count()`: counts the number of occurrences for each combination of 'above' and 'is_match' values  \n",
    "- The resulting cross-tabulation DataFrame is returned  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:\n",
    "    return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calling the `crossTabs` function with a threshold of 4.0 and displaying the result**  \n",
    "- `crossTabs(scored, 4.0)`: calls the `crossTabs` function with the `scored` DataFrame and a threshold value of 4.0  \n",
    "  - The function creates a cross-tabulation DataFrame based on the 'score' and 'is_match' columns, using a threshold of 4.0 for the 'above' column  \n",
    "- `.show()`: displays the resulting cross-tabulation DataFrame  \n",
    "  \n",
    "The output will be a cross-tabulation DataFrame with the following columns:  \n",
    "- 'above': indicates whether the 'score' is greater than or equal to 4.0 (true or false)  \n",
    "- 'true': count of occurrences where 'is_match' is true  \n",
    "- 'false': count of occurrences where 'is_match' is false  \n",
    "  \n",
    "The DataFrame will have two rows:  \n",
    "- Row 1: counts for 'score' values less than 4.0  \n",
    "- Row 2: counts for 'score' values greater than or equal to 4.0  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossTabs(scored, 4.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calling the `crossTabs` function with a threshold of 2.0 and displaying the result**  \n",
    "- `crossTabs(scored, 2.0)`: calls the `crossTabs` function with the `scored` DataFrame and a threshold value of 2.0  \n",
    "  - The function creates a cross-tabulation DataFrame based on the 'score' and 'is_match' columns, using a threshold of 2.0 for the 'above' column  \n",
    "- `.show()`: displays the resulting cross-tabulation DataFrame  \n",
    "  \n",
    "The output will be a cross-tabulation DataFrame with the following columns:  \n",
    "- 'above': indicates whether the 'score' is greater than or equal to 2.0 (true or false)  \n",
    "- 'true': count of occurrences where 'is_match' is true  \n",
    "- 'false': count of occurrences where 'is_match' is false  \n",
    "  \n",
    "The DataFrame will have two rows:  \n",
    "- Row 1: counts for 'score' values less than 2.0  \n",
    "- Row 2: counts for 'score' values greater than or equal to 2.0  \n",
    "  \n",
    "This cross-tabulation provides a summary of the counts for different combinations of 'above' and 'is_match' values, allowing for analysis of the relationship between the 'score' and 'is_match' columns based on the specified threshold of 2.0.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossTabs(scored, 2.0).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
