{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a SparkContext or handle existing context**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "try:\n",
    "    sc = ps.SparkContext('local[*]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the unittest module, which provides a framework for writing and running unit tests in Python.  \n",
    "  \n",
    "Imports the sys module, which provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.  \n",
    "  \n",
    "**class TestRdd(unittest.TestCase):**  \n",
    "Defines a test case class named TestRdd, which inherits from unittest.TestCase. This class will contain test methods for testing the functionality of an RDD (Resilient Distributed Dataset) in a distributed computing framework like Apache Spark.  \n",
    "  \n",
    "**def test_take(self):**  \n",
    "Defines a test method named test_take within the TestRdd class. This method tests the take() operation on an RDD, which returns the first n elements of the RDD.  \n",
    "  \n",
    "**input = sc.parallelize([1,2,3,4])**  \n",
    "Creates an RDD named input by parallelizing the list [1, 2, 3, 4] using the sc object, which is likely a SparkContext instance.  \n",
    "  \n",
    "**self.assertEqual([1,2,3,4], input.take(4))**  \n",
    "Asserts that the result of calling take(4) on the input RDD is equal to the list [1, 2, 3, 4]. If the assertion fails, the test case will fail.  \n",
    "  \n",
    "**def run_tests():**  \n",
    "Defines a function named run_tests that runs the test suite.  \n",
    "  \n",
    "**suite = unittest.TestLoader().loadTestsFromTestCase( TestRdd )**  \n",
    "Creates a test suite by loading all test cases from the TestRdd class using the unittest.TestLoader.  \n",
    "  \n",
    "**unittest.TextTestRunner(verbosity=1,stream=sys.stderr).run( suite )**  \n",
    "Runs the test suite using a TextTestRunner with verbosity level 1 (prints a dot for each successful test) and directs the output to sys.stderr.  \n",
    "  \n",
    "**run_tests()**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import sys\n",
    "\n",
    "class TestRdd(unittest.TestCase):\n",
    "    def test_take(self):\n",
    "        input = sc.parallelize([1,2,3,4])\n",
    "        self.assertEqual([1,2,3,4], input.take(4))\n",
    "\n",
    "def run_tests():\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase( TestRdd )\n",
    "    unittest.TextTestRunner(verbosity=1,stream=sys.stderr).run( suite )\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`import pyspark`**: This line imports the PySpark library, which is the Python API for Apache Spark.  \n",
    "  \n",
    "**`from pyspark.sql import SparkSession`**: This line imports the `SparkSession` class from the `pyspark.sql` module, which is used to create a Spark session.  \n",
    "  \n",
    "**`def start():`**: This defines a function named `start()` that creates a Spark session.  \n",
    "  \n",
    "**`spark = SparkSession.builder \\`**  \n",
    "**`.appName(\"DataFrameExample\") \\`**  \n",
    "**`.getOrCreate()`**: These lines create a new Spark session with the application name \"DataFrameExample\". The `getOrCreate()` method ensures that if a Spark session already exists, it returns that session instead of creating a new one.  \n",
    "  \n",
    "**`return spark`**: This line returns the created Spark session.  \n",
    "  \n",
    "**`sc = start()`**: This line calls the `start()` function and assigns the returned Spark session to the variable `sc`.  \n",
    "  \n",
    "**`data = [(...), ...]`**: This is a list of tuples representing the data to be used for creating a Spark DataFrame.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the json module, which provides functionality for parsing and working with JSON data.  \n",
    "\n",
    "```python  \n",
    "fields = ['product_id', 'user_id', 'score', 'time']  \n",
    "fields2 = ['product_id', 'user_id', 'review', 'profile_name', 'helpfulness', 'score', 'time']  \n",
    "fields3 = ['product_id', 'user_id', 'time']  \n",
    "fields4 = ['user_id', 'score', 'time']  \n",
    "```\n",
    "Defines lists of field names that are expected in the JSON data.  \n",
    "  \n",
    "Define a function `validate` that takes a line and checks if it contains all the fields specified in `fields2`. If any field is missing, it returns `False`, otherwise it returns `True`.  \n",
    "  \n",
    "```python\n",
    "reviews_raw = sc.textFile('data/movies.json')\n",
    "```\n",
    "Reads the contents of the file 'data/movies.json' into an RDD (Resilient Distributed Dataset) named `reviews_raw`.  \n",
    "  \n",
    "```python\n",
    "reviews = reviews_raw.map(lambda line: json.loads(line)).filter(validate)\n",
    "```  \n",
    "Applies two transformations to the `reviews_raw` RDD:  \n",
    "1. `map(lambda line: json.loads(line))`: Converts each line (string) into a Python dictionary using `json.loads`.  \n",
    "2. `filter(validate)`: Filters out any dictionaries that do not pass the `validate` function.  \n",
    "The resulting RDD is stored in `reviews`.  \n",
    "  \n",
    "```python\n",
    "reviews.cache()\n",
    "```  \n",
    "Caches the `reviews` RDD in memory for faster access.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fields = ['product_id', 'user_id', 'score', 'time']\n",
    "fields2 = ['product_id', 'user_id', 'review', 'profile_name', 'helpfulness', 'score', 'time']\n",
    "fields3 = ['product_id', 'user_id', 'time']\n",
    "fields4 = ['user_id', 'score', 'time']\n",
    "def validate(line):\n",
    "    for field in fields2:\n",
    "        if field not in line: \n",
    "            return False\n",
    "    return True\n",
    "\n",
    "reviews_raw = sc.textFile('data/movies.json')\n",
    "reviews = reviews_raw.map(lambda line: json.loads(line)).filter(validate)\n",
    "reviews.cache()\n",
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_movies = reviews.groupBy(lambda entry: entry['product_id']).count()\n",
    "```\n",
    "This line groups the reviews DataFrame by the 'product_id' column (which represents movies), and counts the number of unique 'product_id' values, giving the total number of movies in the dataset.  \n",
    "  \n",
    "```python\n",
    "num_users = reviews.groupBy(lambda entry: entry['user_id']).count()\n",
    "```\n",
    "This line groups the reviews DataFrame by the 'user_id' column, and counts the number of unique 'user_id' values, giving the total number of users in the dataset.  \n",
    "  \n",
    "```python\n",
    "num_entries = reviews.count()\n",
    "```\n",
    "This line counts the total number of rows (reviews) in the reviews DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_movies = reviews.groupBy(lambda entry: entry['product_id']).count()\n",
    "num_users = reviews.groupBy(lambda entry: entry['user_id']).count()\n",
    "num_entries = reviews.count()\n",
    "print (str(num_entries) + \" reviews of \" + str(num_movies) + \" movies by \" + str(num_users) + \" different people.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line filters the `reviews` RDD to only include entries where the `user_id` key exists and has a non-empty value.  \n",
    "  \n",
    "```python\n",
    "r1 = reviews.map(lambda r: ((r['product_id'],), 1))\n",
    "```\n",
    "\n",
    "This line creates a new RDD `r1` by mapping each review in the `reviews` RDD to a tuple containing the `product_id` as the key and the value 1.  \n",
    "  \n",
    "```python\n",
    "avg3 = r1.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "```\n",
    "\n",
    "This line performs a map and reduce operation on the `r1` RDD. It first maps each value (which is 1) to a tuple of (value, 1), then reduces by key, summing up the values and counts for each `product_id`.  \n",
    "  \n",
    "```python\n",
    "avg3 = avg3.filter(lambda x: x[1][1] > 20 )\n",
    "```\n",
    "\n",
    "This line filters the `avg3` RDD to only include `product_id`s that have been reviewed by more than 20 users.  \n",
    "  \n",
    "```python\n",
    "avg3 = avg3.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False)\n",
    "```\n",
    "\n",
    "This line maps the `avg3` RDD to swap the key and value, with the new key being the sum of the count and value, and the value being the `product_id`. It then sorts the RDD by the new key in descending order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suggestion_users = reviews.filter(lambda entry: entry['user_id'])\n",
    "#for review in Suggestion_users.collect():\n",
    "r1 = reviews.map(lambda r: ((r['product_id'],), 1))\n",
    "avg3 = r1.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "avg3 = avg3.filter(lambda x: x[1][1] > 20 )\n",
    "avg3 = avg3.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False)\n",
    "\n",
    "for movie in avg3.take(10):\n",
    "    print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" WATCHED BY : \" + str(movie[0][0]) + \" PEOPLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "r2 = reviews.map(lambda ru: ((ru['user_id'],), 1))\n",
    "```\n",
    "Creates an RDD `r2` by mapping each review `ru` to a tuple containing the user ID as a single-element tuple and the value 1.  \n",
    "  \n",
    "```python\n",
    "avg2 = r2.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1]+ y[1]))\n",
    "```  \n",
    "Transforms `r2` by adding a count of 1 to each value, then reduces by key, summing the values and counts for each user ID.  \n",
    "  \n",
    "```python\n",
    "avg2 = avg2.filter(lambda x: x[1][1] > 20 )\n",
    "```  \n",
    "Filters `avg2` to include only user IDs with a count greater than 20.  \n",
    "  \n",
    "```python\n",
    "avg2 = avg2.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False)\n",
    "```  \n",
    "Transforms `avg2` by adding the value and count for each user ID, then sorts the resulting RDD by the summed value in descending order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = reviews.map(lambda ru: ((ru['user_id'],), 1))\n",
    "avg2 = r2.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1]+ y[1]))\n",
    "avg2 = avg2.filter(lambda x: x[1][1] > 20 )\n",
    "avg2 = avg2.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False)\n",
    "\n",
    "for movie in avg2.take(10):\n",
    "    print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" WATCHED : \" + str(movie[0][0]) + \" MOVIES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "```python\n",
    "filtered = reviews.filter(lambda entry: \"George\" in entry['profile_name'])\n",
    "```\n",
    "This line filters the `reviews` dataset to only include entries where the `profile_name` field contains the string \"George\".  \n",
    "  \n",
    "```python\n",
    "print (\"Found \" + str(filtered.count()) + \" entries.\\n\")\n",
    "```  \n",
    "Prints the number of entries found after filtering.  \n",
    "  \n",
    "```  python\n",
    "for review in filtered.collect():  \n",
    "    print (\"Rating: \" + str(review['score']) + \" and helpfulness: \" + review['helpfulness'])  \n",
    "    print (\"http://www.amazon.com/dp/\" + review['product_id'])  \n",
    "    print (review['summary'])  \n",
    "    print (review['review'])  \n",
    "    print (\"\\n\")  \n",
    "```  \n",
    "This loop iterates over each review in the filtered dataset and prints:  \n",
    "- The rating score and helpfulness score  \n",
    "- The Amazon product URL  \n",
    "- The review summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has someone written a review?\n",
    "filtered = reviews.filter(lambda entry: \"George\" in entry['profile_name'])\n",
    "print (\"Found \" + str(filtered.count()) + \" entries.\\n\")\n",
    "for review in filtered.collect():\n",
    "    print (\"Rating: \" + str(review['score']) + \" and helpfulness: \" + review['helpfulness'])\n",
    "    print (\"http://www.amazon.com/dp/\" + review['product_id'])\n",
    "    print (review['summary'])\n",
    "    print (review['review'])\n",
    "    print (\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "```python  \n",
    "reviews_by_movie = reviews.map(lambda r: ((r['product_id'],), r['score']))  \n",
    "```  \n",
    "This line groups the reviews by movie ID and associates each movie ID with its corresponding review score.  \n",
    "  \n",
    "```python  \n",
    "avg = reviews_by_movie.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1]+ y[1]))  \n",
    "```  \n",
    "This line calculates the average score for each movie by summing up the scores and counts, and then dividing the sum of scores by the sum of counts.  \n",
    "  \n",
    "```python  \n",
    "avg = avg.filter(lambda x: x[1][1] > 20 )  \n",
    "```  \n",
    "This line filters out movies that have fewer than 20 reviews, ensuring that the average scores are based on a sufficient number of reviews.  \n",
    "  \n",
    "```python  \n",
    "avg = avg.map(lambda x: ((x[1][0]/x[1][1],), x[0])).sortByKey(ascending=True)  \n",
    "```  \n",
    "This line creates a new RDD with the average score as the key and the movie ID as the value, and then sorts the RDD by the average score in ascending order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best and worst rated movies\n",
    "reviews_by_movie = reviews.map(lambda r: ((r['product_id'],), r['score']))\n",
    "avg = reviews_by_movie.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1]+ y[1]))\n",
    "avg = avg.filter(lambda x: x[1][1] > 20 )\n",
    "avg = avg.map(lambda x: ((x[1][0]/x[1][1],), x[0])).sortByKey(ascending=True)\n",
    "\n",
    "for movie in avg.take(10):\n",
    "    print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" Rating: \" + str(movie[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the `datetime` module from the Python standard library, which provides classes for working with dates and times.  \n",
    "  \n",
    "```python\n",
    "timeseries_rdd = reviews.map(lambda entry: {'score': entry['score'], 'time': datetime.fromtimestamp(entry['time'])})\n",
    "```  \n",
    "- `reviews` is assumed to be an RDD (Resilient Distributed Dataset) containing review entries.  \n",
    "- The `map` transformation is applied to the `reviews` RDD, which applies the provided lambda function to each entry in the RDD.  \n",
    "- The lambda function `lambda entry: {'score': entry['score'], 'time': datetime.fromtimestamp(entry['time'])}` creates a new dictionary for each entry with two keys:  \n",
    "  - `'score'`: The value of the `'score'` key from the original entry.  \n",
    "  - `'time'`: A `datetime` object created from the `'time'` value of the original entry, which is assumed to be a Unix timestamp. The `datetime.fromtimestamp` function is used to convert the Unix timestamp to a `datetime` object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timeseries_rdd = reviews.map(lambda entry: {'score': entry['score'], 'time': datetime.fromtimestamp(entry['time'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample data from an RDD**  \n",
    "The code samples 20,000 entries from the `timeseries_rdd` RDD (Resilient Distributed Dataset) without replacement and with a specific seed.  \n",
    "  \n",
    "**Create a pandas DataFrame**  \n",
    "The sampled data is converted into a pandas DataFrame with columns 'score' and 'time'.  \n",
    "  \n",
    "**Print the first 3 rows of the DataFrame**  \n",
    "The `head(3)` method is used to print the first 3 rows of the DataFrame.  \n",
    "  \n",
    "**Convert the 'score' column to float64 data type**  \n",
    "The `astype` method is used to convert the 'score' column to float64 data type.  \n",
    "  \n",
    "**Set the 'time' column as the index**  \n",
    "The `set_index` method is used to set the 'time' column as the index of the DataFrame, with `inplace=True` to modify the DataFrame in place.  \n",
    "  \n",
    "**Resample and plot the data**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "sample = timeseries_rdd.sample(withReplacement=False, fraction=20000.0/num_entries, seed=1134)\n",
    "timeseries = pd.DataFrame(sample.collect(), columns=['score', 'time'])\n",
    "\n",
    "print(timeseries.head(3))\n",
    "timeseries.score.astype('float64')\n",
    "timeseries.set_index('time', inplace=True)\n",
    "\n",
    "Rsample = timeseries.score.resample('Y').count()\n",
    "Rsample.plot()\n",
    "Rsample2 = timeseries.score.resample('M').count()\n",
    "Rsample2.plot()\n",
    "Rsample3 = timeseries.score.resample('Q').count()\n",
    "Rsample3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg.take(4):\n",
    "plt.bar(movie[1][0],movie[0][0])\n",
    "plt.title('Histogram of \\'AVERAGE RATING OF MOVIE\\'')\n",
    "plt.xlabel('MOVIE')\n",
    "plt.ylabel('AVGRATING')\n",
    "\n",
    "for movie in avg2.take(3):\n",
    "plt.bar(movie[1][0],movie[0][0])\n",
    "plt.title('Histogram of \\'NUMBER OF MOVIES REVIEWED BY USER\\'')\n",
    "plt.xlabel('USER')\n",
    "plt.ylabel('MOVIE COUNT')\n",
    "\n",
    "for movie in avg3.take(4):\n",
    "plt.bar(movie[1][0],movie[0][0])\n",
    "plt.title('Histogram of \\'MOVIES REVIEWED BY NUMBER OF USERS\\'')\n",
    "plt.xlabel('MOVIE')\n",
    "plt.ylabel('USER COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the ALS (Alternating Least Squares) class from the pyspark.mllib.recommendation module, which is used for collaborative filtering.  \n",
    "  \n",
    "```python\n",
    "def get_hash(s):\n",
    "    return int(hashlib.sha1(s).hexdigest(), 16) % (10 ** 8)**  \n",
    "```  \n",
    "Defines a function get_hash that takes a string s as input, hashes it using the SHA-1 algorithm, converts the hexadecimal digest to an integer, and returns the integer modulo 10^8.  \n",
    "  \n",
    "```python\n",
    "ratings = reviews.map(lambda entry: tuple([ get_hash(entry['user_id'].encode('utf-8')), get_hash(entry['product_id'].encode('utf-8')), int(entry['score']) ]))\n",
    "```  \n",
    "Maps each entry in the reviews dataset to a tuple containing the hashed user_id, hashed product_id, and the score as an integer.  \n",
    "\n",
    "```python  \n",
    "train_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) >=2 )  \n",
    "test_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) < 2 )  \n",
    "```\n",
    "Splits the ratings data into train_data and test_data based on the sum of the hashed user_id and hashed product_id modulo 10. If the result is greater than or equal to 2, the entry is added to train_data, otherwise, it is added to test_data.  \n",
    "  \n",
    "```python\n",
    "train_data.cache()\n",
    "```\n",
    "Caches the train_data dataset in memory for faster access.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "from numpy import array\n",
    "import hashlib\n",
    "import math\n",
    "def get_hash(s):\n",
    "    return int(hashlib.sha1(s).hexdigest(), 16) % (10 ** 8)\n",
    "\n",
    "#Input format: [user, product, rating]\n",
    "ratings = reviews.map(lambda entry: tuple([ get_hash(entry['user_id'].encode('utf-8')),\n",
    "get_hash(entry['product_id'].encode('utf-8')), int(entry['score']) ]))\n",
    "train_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) >=2 )\n",
    "test_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) < 2 )\n",
    "train_data.cache()\n",
    "\n",
    "#train_data.union(train_data)\n",
    "print (\"Number of train samples: \" + str(train_data.count()))\n",
    "print (\"Number of test samples: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rank = 20` sets the rank of the factorized user-item matrices to 20  \n",
    "- `numIterations = 20` sets the number of iterations for the ALS algorithm to 20  \n",
    "- `model = ALS.train(train_data, rank, numIterations)` trains the ALS recommendation model on the `train_data` with the specified rank and number of iterations  \n",
    "  \n",
    "**Helper function to convert strings to floats**  \n",
    "- `convertToFloat(lines)` takes a list of strings `lines` and converts each element to a float, returning a new list with the float values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using Alternating Least Squares\n",
    "from math import sqrt\n",
    "rank = 20\n",
    "numIterations = 20\n",
    "model = ALS.train(train_data, rank, numIterations)\n",
    "\n",
    "def convertToFloat(lines):\n",
    "    returnedLine = []\n",
    "    for x in lines:\n",
    "        returnedLine.append(float(x))\n",
    "    return returnedLine\n",
    "\n",
    "# Evaluate the model on test data\n",
    "unknown = test_data.map(lambda entry: (int(entry[0]), int(entry[1])))\n",
    "predictions = model.predictAll(unknown).map(lambda r: ((int(r[0]), int(r[1])), r[2]))\n",
    "true_and_predictions = test_data.map(lambda r: ((int(r[0]), int(r[1])), r[2])).join(predictions)\n",
    "MSE = true_and_predictions.map(lambda r: (int(r[1][0]) - int(r[1][1])**2).reduce(lambda x, y: x + y)/true_and_predictions.count())\n",
    "true_and_predictions.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line sets a minimum occurrence threshold for words to be considered in the analysis.  \n",
    "  \n",
    "**good_reviews = reviews.filter(lambda line: line['score']==5.0)**  \n",
    "This line filters the reviews dataset to only include reviews with a score of 5.0 (presumably positive reviews).  \n",
    "  \n",
    "**bad_reviews = reviews.filter(lambda line: line['score']==1.0)**  \n",
    "This line filters the reviews dataset to only include reviews with a score of 1.0 (presumably negative reviews).  \n",
    "\n",
    "```python  \n",
    "good_words = good_reviews.flatMap(lambda line: line['review'].split(' '))  \n",
    "num_good_words = good_words.count()  \n",
    "good_words = good_words.map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a+b).filter(lambda word_count: word_count[1] > min_occurrences)  \n",
    "```\n",
    "These lines extract the words from the positive reviews, count the total number of words, and then count the occurrences of each word, keeping only those that occur more than the minimum occurrence threshold.  \n",
    "\n",
    "```python  \n",
    "bad_words = bad_reviews.flatMap(lambda line: line['review'].split(' '))  \n",
    "num_bad_words = bad_words.count()  \n",
    "bad_words = bad_words.map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a+b).filter(lambda word_count: word_count[1] > min_occurrences)  \n",
    "```\n",
    "These lines perform the same operations as above, but for the negative reviews.  \n",
    "\n",
    "```python\n",
    "frequency_good = good_words.map(lambda word: ((word[0],), float(word[1])/num_good_words))  \n",
    "frequency_bad = bad_words.map(lambda word: ((word[0],), float(word[1])/num_bad_words))  \n",
    "```\n",
    "These lines calculate the frequency of each word in the positive and negative reviews, respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurrences = 10\n",
    "good_reviews = reviews.filter(lambda line: line['score']==5.0)\n",
    "bad_reviews = reviews.filter(lambda line: line['score']==1.0)\n",
    "\n",
    "good_words = good_reviews.flatMap(lambda line: line['review'].split(' '))\n",
    "num_good_words = good_words.count()\n",
    "good_words = good_words.map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a+b).filter(lambda word_count: word_count[1] > min_occurrences)\n",
    "bad_words = bad_reviews.flatMap(lambda line: line['review'].split(' '))\n",
    "num_bad_words = bad_words.count()\n",
    "bad_words = bad_words.map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a+b).filter(lambda word_count: word_count[1] > min_occurrences)\n",
    "\n",
    "frequency_good = good_words.map(lambda word: ((word[0],), float(word[1])/num_good_words))\n",
    "frequency_bad = bad_words.map(lambda word: ((word[0],), float(word[1])/num_bad_words))\n",
    "\n",
    "joined_frequencies = frequency_good.join(frequency_bad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def relative_difference(a, b):  \n",
    "    return math.fabs(a-b)/a  \n",
    "```\n",
    "Defines a function `relative_difference` that calculates the relative difference between two numbers `a` and `b`. It uses the `math.fabs` function to get the absolute value of the difference between `a` and `b`, and then divides it by `a`.  \n",
    "\n",
    "```python  \n",
    "result = joined_frequencies.map(lambda f: ((relative_difference(f[1][0],f[1][1]),), f[0][0]) ).sortByKey(ascending=False)  \n",
    "```\n",
    "Applies a map transformation to the `joined_frequencies` dataset. For each element `f` in the dataset, it calculates the relative difference between the two values in `f[1]` (presumably frequencies) using the `relative_difference` function. It then creates a tuple with the relative difference as the first element and the key `f[0][0]` as the second element. The resulting tuples are sorted in descending order based on the relative difference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def relative_difference(a, b):\n",
    "    return math.fabs(a-b)/a\n",
    "result = joined_frequencies.map(lambda f: ((relative_difference(f[1][0],f[1][1]),), f[0][0]) ).sortByKey(ascending=False)\n",
    "result.take(50)\n",
    "\n",
    "for movie in result.take(7):\n",
    "    plt.bar(movie[1],movie[0][0])\n",
    "    plt.title('Histogram of \\'SENTIMENT ANALYSIS\\'')\n",
    "    plt.xlabel('WORD')\n",
    "    plt.ylabel('NUMBER OF OCCURANCES')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
